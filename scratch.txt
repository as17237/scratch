-----------------api_clients----------------------------
import requests
import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import json

@st.cache_data(ttl=300)  # Cache for 5 minutes
def fetch_weather_data(city="London"):
    """Fetch weather data from OpenWeatherMap API (mock implementation)."""
    try:
        # Mock weather data since we don't have API keys
        weather_data = {
            'London': {'temp': 18, 'humidity': 65, 'description': 'Cloudy'},
            'New York': {'temp': 22, 'humidity': 55, 'description': 'Sunny'},
            'Tokyo': {'temp': 25, 'humidity': 70, 'description': 'Rainy'},
            'Sydney': {'temp': 28, 'humidity': 60, 'description': 'Clear'},
            'Paris': {'temp': 20, 'humidity': 68, 'description': 'Partly Cloudy'}
        }
        
        return weather_data.get(city, {'temp': 20, 'humidity': 60, 'description': 'Unknown'})
    except Exception as e:
        st.error(f"Error fetching weather data: {e}")
        return None

@st.cache_data(ttl=600)  # Cache for 10 minutes
def fetch_stock_data(symbol="AAPL", days=30):
    """Fetch stock data (mock implementation)."""
    try:
        # Generate mock stock data
        import numpy as np
        np.random.seed(42)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        dates = pd.date_range(start=start_date, end=end_date, freq='D')
        
        # Generate realistic stock price movement
        base_price = 150.0
        returns = np.random.normal(0, 0.02, len(dates))  # 2% daily volatility
        prices = [base_price]
        
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        data = {
            'date': dates,
            'open': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
            'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'close': prices,
            'volume': np.random.poisson(1000000, len(dates))
        }
        
        return pd.DataFrame(data)
    except Exception as e:
        st.error(f"Error fetching stock data: {e}")
        return None

@st.cache_data(ttl=1800)  # Cache for 30 minutes
def fetch_news_data(category="technology", limit=10):
    """Fetch news data (mock implementation)."""
    try:
        # Mock news data
        news_templates = {
            'technology': [
                "New AI breakthrough in {topic}",
                "{company} releases revolutionary {product}",
                "Tech industry sees major changes in {sector}",
                "Innovation in {field} promises future growth"
            ],
            'business': [
                "{company} reports strong Q4 earnings",
                "Market trends show growth in {sector}",
                "New regulations impact {industry}",
                "Investment opportunities in {field}"
            ],
            'science': [
                "Breakthrough discovery in {field}",
                "Research team finds new {discovery}",
                "Scientific advancement in {area}",
                "Study reveals insights about {topic}"
            ]
        }
        
        import random
        topics = ['machine learning', 'blockchain', 'quantum computing', 'biotechnology', 'renewable energy']
        companies = ['TechCorp', 'InnovateInc', 'FutureTech', 'GlobalSystems', 'NextGen']
        sectors = ['finance', 'healthcare', 'education', 'transportation', 'energy']
        
        news_items = []
        templates = news_templates.get(category, news_templates['technology'])
        
        for i in range(limit):
            template = random.choice(templates)
            title = template.format(
                topic=random.choice(topics),
                company=random.choice(companies),
                product=random.choice(topics),
                sector=random.choice(sectors),
                field=random.choice(topics),
                industry=random.choice(sectors),
                discovery=random.choice(topics),
                area=random.choice(topics)
            )
            
            news_items.append({
                'title': title,
                'summary': f"This is a summary of the news about {title.lower()}.",
                'published_at': datetime.now() - timedelta(hours=random.randint(1, 72)),
                'source': random.choice(['TechNews', 'BusinessDaily', 'ScienceToday', 'InnovationHub']),
                'url': f"https://example.com/news/{i+1}"
            })
        
        return pd.DataFrame(news_items)
    except Exception as e:
        st.error(f"Error fetching news data: {e}")
        return None

@st.cache_data(ttl=3600)  # Cache for 1 hour
def fetch_covid_data(country="US"):
    """Fetch COVID-19 data (mock implementation)."""
    try:
        # Generate mock COVID data
        import numpy as np
        np.random.seed(42)
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=90)
        dates = pd.date_range(start=start_date, end=end_date, freq='D')
        
        # Generate realistic COVID data with trends
        base_cases = 1000
        cases = []
        deaths = []
        recovered = []
        
        for i, date in enumerate(dates):
            # Add some trend and seasonality
            trend_factor = 1 + 0.1 * np.sin(i * 2 * np.pi / 30)  # Monthly cycle
            random_factor = np.random.normal(1, 0.2)
            
            daily_cases = int(base_cases * trend_factor * random_factor)
            cases.append(daily_cases)
            deaths.append(int(daily_cases * 0.02 * np.random.normal(1, 0.3)))  # ~2% death rate
            recovered.append(int(daily_cases * 0.8 * np.random.normal(1, 0.2)))  # ~80% recovery rate
        
        data = {
            'date': dates,
            'new_cases': cases,
            'new_deaths': deaths,
            'new_recovered': recovered,
            'total_cases': np.cumsum(cases),
            'total_deaths': np.cumsum(deaths),
            'total_recovered': np.cumsum(recovered)
        }
        
        return pd.DataFrame(data)
    except Exception as e:
        st.error(f"Error fetching COVID data: {e}")
        return None

def make_api_request(url, params=None, headers=None, timeout=10):
    """Generic function to make API requests with error handling."""
    try:
        response = requests.get(url, params=params, headers=headers, timeout=timeout)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"API request failed: {e}")
        return None
    except json.JSONDecodeError as e:
        st.error(f"Failed to parse JSON response: {e}")
        return None

@st.cache_data(ttl=300)
def fetch_user_analytics(user_id=None):
    """Fetch user analytics data (mock implementation)."""
    try:
        import numpy as np
        np.random.seed(42)
        
        # Generate mock user analytics
        metrics = {
            'total_users': np.random.randint(10000, 50000),
            'active_users': np.random.randint(5000, 15000),
            'new_signups': np.random.randint(100, 500),
            'page_views': np.random.randint(50000, 200000),
            'avg_session_duration': np.random.randint(120, 600),  # seconds
            'bounce_rate': np.random.uniform(0.3, 0.7),
            'conversion_rate': np.random.uniform(0.01, 0.05)
        }
        
        return metrics
    except Exception as e:
        st.error(f"Error fetching user analytics: {e}")
        return None 


---------------------------------------cache_utils-------------------------------------------------
import streamlit as st
import pandas as pd
import numpy as np
from functools import wraps
import time
import hashlib
import json

def cache_with_key_generator(key_func):
    """Decorator that allows custom key generation for caching."""
    def decorator(func):
        @st.cache_data
        def cached_func(*args, **kwargs):
            # Generate custom key
            custom_key = key_func(*args, **kwargs)
            # Use the custom key as part of the cache key
            return func(*args, **kwargs)
        return cached_func
    return decorator

def hash_dataframe(df):
    """Generate a hash for a dataframe to use as cache key."""
    return hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()

def cache_dataframe_operation(operation_name):
    """Cache decorator specifically for dataframe operations."""
    def decorator(func):
        @st.cache_data
        def cached_func(df, *args, **kwargs):
            # Include dataframe hash in cache key
            df_hash = hash_dataframe(df)
            return func(df, *args, **kwargs)
        return cached_func
    return decorator

@st.cache_data(ttl=3600)  # Cache for 1 hour
def expensive_calculation(data, parameters):
    """Example of expensive calculation with caching."""
    # Simulate expensive computation
    time.sleep(0.1)
    
    result = {
        'mean': np.mean(data),
        'std': np.std(data),
        'percentiles': np.percentile(data, [25, 50, 75]),
        'parameters': parameters
    }
    
    return result

@st.cache_data(ttl=1800)  # Cache for 30 minutes
def load_large_dataset(file_path):
    """Load large dataset with caching."""
    # Simulate loading large dataset
    time.sleep(0.5)
    
    # Generate mock large dataset
    data = {
        'id': range(10000),
        'value': np.random.normal(100, 20, 10000),
        'category': np.random.choice(['A', 'B', 'C', 'D'], 10000),
        'timestamp': pd.date_range(start='2023-01-01', periods=10000, freq='H')
    }
    
    return pd.DataFrame(data)

@st.cache_data(ttl=600)  # Cache for 10 minutes
def compute_aggregations(df, group_by_columns, agg_functions):
    """Compute aggregations with caching."""
    # Simulate expensive aggregation
    time.sleep(0.2)
    
    return df.groupby(group_by_columns).agg(agg_functions).reset_index()

@st.cache_data(ttl=300)  # Cache for 5 minutes
def fetch_external_data(source, parameters):
    """Fetch external data with caching."""
    # Simulate API call
    time.sleep(0.3)
    
    # Mock external data
    data = {
        'source': source,
        'timestamp': pd.Timestamp.now(),
        'data': np.random.rand(100),
        'parameters': parameters
    }
    
    return data

def clear_cache_for_function(func_name):
    """Clear cache for a specific function."""
    try:
        # This is a simplified version - in practice, you'd need to access Streamlit's cache
        st.cache_data.clear()
        st.success(f"Cache cleared for {func_name}")
    except Exception as e:
        st.error(f"Failed to clear cache: {e}")

def get_cache_info():
    """Get information about cache usage (mock implementation)."""
    return {
        'cache_size': '2.5 MB',
        'cache_hits': 156,
        'cache_misses': 23,
        'hit_rate': '87.2%',
        'functions_cached': 8
    }

@st.cache_data(ttl=1200)  # Cache for 20 minutes
def process_image_data(image_data, processing_params):
    """Process image data with caching."""
    # Simulate image processing
    time.sleep(0.4)
    
    # Mock image processing result
    result = {
        'processed_image': image_data,  # In real app, this would be processed image
        'metadata': {
            'size': len(image_data),
            'processing_time': 0.4,
            'parameters': processing_params
        }
    }
    
    return result

@st.cache_data(ttl=900)  # Cache for 15 minutes
def generate_report_data(report_type, date_range, filters):
    """Generate report data with caching."""
    # Simulate report generation
    time.sleep(0.6)
    
    # Mock report data
    report_data = {
        'report_type': report_type,
        'date_range': date_range,
        'filters': filters,
        'summary': {
            'total_records': np.random.randint(1000, 10000),
            'total_value': np.random.uniform(100000, 1000000),
            'growth_rate': np.random.uniform(-0.1, 0.3)
        },
        'details': pd.DataFrame({
            'category': ['A', 'B', 'C', 'D'],
            'value': np.random.rand(4) * 1000,
            'percentage': np.random.rand(4)
        })
    }
    
    return report_data

def cache_performance_monitor():
    """Monitor cache performance (mock implementation)."""
    return {
        'total_requests': 245,
        'cache_hits': 198,
        'cache_misses': 47,
        'hit_rate': '80.8%',
        'avg_response_time': '0.15s',
        'cache_size': '1.8 MB',
        'memory_usage': '12.3%'
    }

@st.cache_data(ttl=300)  # Cache for 5 minutes
def get_user_preferences(user_id):
    """Get user preferences with caching."""
    # Simulate database query
    time.sleep(0.1)
    
    # Mock user preferences
    preferences = {
        'theme': 'dark',
        'language': 'en',
        'timezone': 'UTC',
        'notifications': True,
        'dashboard_layout': 'grid'
    }
    
    return preferences

def invalidate_cache_pattern(pattern):
    """Invalidate cache entries matching a pattern (mock implementation)."""
    st.info(f"Cache entries matching '{pattern}' would be invalidated")
    # In a real implementation, you would clear specific cache entries
    return True 

















------------------------------------------data_generator.py---------------------------------------------
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

@st.cache_data
def generate_sample_sales_data(rows=1000):
    """Generate sample sales data with caching for performance."""
    np.random.seed(42)
    
    # Generate dates
    start_date = datetime(2023, 1, 1)
    dates = [start_date + timedelta(days=i) for i in range(rows)]
    
    # Generate product categories
    categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports']
    
    # Generate regions
    regions = ['North', 'South', 'East', 'West', 'Central']
    
    data = {
        'date': dates,
        'product_category': np.random.choice(categories, rows),
        'region': np.random.choice(regions, rows),
        'sales_amount': np.random.normal(1000, 300, rows),
        'quantity_sold': np.random.poisson(50, rows),
        'customer_rating': np.random.uniform(1, 5, rows),
        'discount_percent': np.random.uniform(0, 30, rows)
    }
    
    df = pd.DataFrame(data)
    df['sales_amount'] = df['sales_amount'].abs()  # Ensure positive values
    df['customer_rating'] = df['customer_rating'].round(1)
    df['discount_percent'] = df['discount_percent'].round(1)
    
    return df

@st.cache_data
def generate_time_series_data(periods=100):
    """Generate time series data for real-time demonstrations."""
    np.random.seed(42)
    
    # Generate time index
    base_time = datetime.now() - timedelta(hours=periods)
    time_index = [base_time + timedelta(minutes=i*15) for i in range(periods)]
    
    # Generate trend with noise
    trend = np.linspace(100, 150, periods)
    noise = np.random.normal(0, 5, periods)
    values = trend + noise
    
    data = {
        'timestamp': time_index,
        'value': values,
        'moving_average': pd.Series(values).rolling(window=5).mean().fillna(method='bfill'),
        'volatility': np.random.exponential(2, periods)
    }
    
    return pd.DataFrame(data)

@st.cache_data
def generate_user_activity_data(users=50, days=30):
    """Generate user activity data for analytics."""
    np.random.seed(42)
    
    user_ids = [f"user_{i:03d}" for i in range(1, users + 1)]
    dates = pd.date_range(start='2023-01-01', periods=days, freq='D')
    
    data = []
    for user_id in user_ids:
        for date in dates:
            # Simulate user activity patterns
            is_weekend = date.weekday() >= 5
            base_activity = 10 if is_weekend else 5
            
            data.append({
                'user_id': user_id,
                'date': date,
                'page_views': np.random.poisson(base_activity),
                'session_duration': np.random.exponential(300),  # seconds
                'bounce_rate': np.random.beta(2, 8),  # 0-1
                'conversion_rate': np.random.beta(1, 20)  # 0-1
            })
    
    return pd.DataFrame(data)

@st.cache_data
def generate_ml_dataset(samples=1000):
    """Generate dataset for machine learning demonstrations."""
    np.random.seed(42)
    
    # Generate features
    age = np.random.normal(35, 12, samples)
    income = np.random.normal(50000, 20000, samples)
    education_years = np.random.normal(14, 3, samples)
    credit_score = np.random.normal(700, 100, samples)
    
    # Generate target (loan approval probability)
    # Higher age, income, education, and credit score increase approval probability
    approval_prob = (
        0.3 + 
        0.2 * (age - 35) / 12 + 
        0.3 * (income - 50000) / 20000 + 
        0.2 * (education_years - 14) / 3 + 
        0.3 * (credit_score - 700) / 100
    )
    
    # Add some noise and ensure probabilities are between 0 and 1
    approval_prob += np.random.normal(0, 0.1, samples)
    approval_prob = np.clip(approval_prob, 0, 1)
    
    # Generate binary target
    loan_approved = (approval_prob > 0.5).astype(int)
    
    data = {
        'age': age.round(1),
        'income': income.round(0),
        'education_years': education_years.round(1),
        'credit_score': credit_score.round(0),
        'approval_probability': approval_prob.round(3),
        'loan_approved': loan_approved
    }
    
    return pd.DataFrame(data)

def generate_random_text(paragraphs=3):
    """Generate random text for file upload demonstrations."""
    lorem_ipsum = """
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. 
    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. 
    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. 
    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    
    Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, 
    eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. 
    Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
    
    Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.
    """
    
    sentences = lorem_ipsum.split('. ')
    random_sentences = random.sample(sentences, min(paragraphs * 3, len(sentences)))
    return '. '.join(random_sentences) + '.' 








------------------------app.py---------------------------------------------
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go

# Page configuration
st.set_page_config(
    page_title="Streamlit Demo Project",
    page_icon="📊",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .feature-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #1f77b4;
    }
    .metric-card {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #e0e0e0;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

def main():
    # Header
    st.markdown('<h1 class="main-header">🚀 Streamlit Demo Project</h1>', unsafe_allow_html=True)
    
    # Introduction
    st.markdown("""
    Welcome to a comprehensive demonstration of Streamlit's capabilities! This project showcases various features 
    and best practices for building interactive data applications.
    """)
    
    # Feature overview
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### 📊 Core Features")
        st.markdown("""
        <div class="feature-card">
            <strong>Interactive Data Visualization</strong><br>
            Charts, graphs, and real-time updates
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <strong>File Upload & Processing</strong><br>
            CSV, image, and text file handling
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <strong>Real-time Data</strong><br>
            Live data fetching and updates
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <strong>Session State Management</strong><br>
            Persistent data across interactions
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("### ⚡ Advanced Features")
        st.markdown("""
        <div class="feature-card">
            <strong>Caching & Performance</strong><br>
            Optimization with @st.cache_data
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <strong>Custom Components</strong><br>
            Reusable UI components
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <strong>API Integration</strong><br>
            External data fetching
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <strong>Machine Learning</strong><br>
            Simple ML model demonstration
        </div>
        """, unsafe_allow_html=True)
    
    # Quick demo section
    st.markdown("---")
    st.markdown("### 🎯 Quick Demo")
    
    # Interactive elements demo
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("#### Interactive Controls")
        slider_value = st.slider("Select a value", 0, 100, 50)
        checkbox_value = st.checkbox("Enable feature")
        select_value = st.selectbox("Choose an option", ["Option A", "Option B", "Option C"])
    
    with col2:
        st.markdown("#### Real-time Metrics")
        st.metric("Current Value", slider_value, delta=slider_value - 50)
        st.metric("Feature Status", "Active" if checkbox_value else "Inactive")
        st.metric("Selected Option", select_value)
    
    with col3:
        st.markdown("#### Sample Chart")
        # Generate sample data
        data = pd.DataFrame({
            'x': np.linspace(0, 10, 100),
            'y': np.sin(np.linspace(0, 10, 100)) * slider_value / 100
        })
        
        fig = px.line(data, x='x', y='y', title="Interactive Sine Wave")
        fig.update_layout(height=200)
        st.plotly_chart(fig, use_container_width=True)
    
    # Session state demo
    st.markdown("---")
    st.markdown("### 💾 Session State Demo")
    
    if 'counter' not in st.session_state:
        st.session_state.counter = 0
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.markdown(f"**Counter Value:** {st.session_state.counter}")
        
        if st.button("Increment Counter"):
            st.session_state.counter += 1
            st.rerun()
        
        if st.button("Reset Counter"):
            st.session_state.counter = 0
            st.rerun()
    
    # Navigation guide
    st.markdown("---")
    st.markdown("### 🧭 Navigation Guide")
    
    st.markdown("""
    Use the sidebar to navigate through different pages:
    
    - **📈 Data Visualization**: Interactive charts and graphs
    - **📁 File Upload**: File processing and analysis
    - **⏰ Real-time Data**: Live data updates and streaming
    - **🤖 Machine Learning**: ML model demonstrations
    - **🧩 Custom Components**: Reusable UI components
    """)
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        Built with ❤️ using Streamlit | 
        <a href='https://streamlit.io' target='_blank'>Learn More</a>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main() 

